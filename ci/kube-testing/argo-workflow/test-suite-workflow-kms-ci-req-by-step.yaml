apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: kms-ci-test-suite-<version>-
  namespace: kms-ci
  labels:
    app: kms-ci-test-suite
spec:
  serviceAccountName: workflow-runner
  activeDeadlineSeconds: 7200  # 2 hours total timeout
  ttlStrategy:
    secondsAfterCompletion: 86400  # Keep for 1 day after completion
  tolerations:
  - key: karpenter.sh/nodepool
    effect: NoSchedule
    operator: Equal
    value: kms-pool
  volumeClaimTemplates:
  - metadata:
      name: test-results-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
  volumes:
  - name: test-results
    persistentVolumeClaim:
      claimName: test-results-data
  - name: slack-webhook-volume
    secret:
      secretName: slack-webhook


  # Main entry point
  entrypoint: kms-ci-test-suite

  templates:
    # Main DAG workflow
    - name: kms-ci-test-suite
      dag:
        tasks:
          ##########################################################################
          # Public decrypt tests
          ##########################################################################
          - name: public-decrypt-euint64-req1
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "public-decrypt-euint64-req1"
                - name: test-command
                  value: "public-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 1"
          ##########################################################################
          # User decrypt tests
          ##########################################################################
          - name: user-decrypt-euint64-req1
            dependencies: ["public-decrypt-euint64-req1"]
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "user-decrypt-euint64-req1"
                - name: test-command
                  value: "user-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 1"

          ##########################################################################
          # Public decrypt tests
          ##########################################################################
          - name: public-decrypt-euint64-req10
            dependencies: ["user-decrypt-euint64-req1"]
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "public-decrypt-euint64-req10"
                - name: test-command
                  value: "public-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 10"

          ##########################################################################
          # User decrypt tests
          ##########################################################################
          - name: user-decrypt-euint64-req10
            dependencies: ["public-decrypt-euint64-req10"]
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "user-decrypt-euint64-req10"
                - name: test-command
                  value: "user-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 10"

          ##########################################################################
          # Public decrypt tests
          ##########################################################################
          - name: public-decrypt-euint64-req100
            dependencies: ["user-decrypt-euint64-req10"]
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "public-decrypt-euint64-req100"
                - name: test-command
                  value: "public-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 100"

          ##########################################################################
          # User decrypt tests
          ##########################################################################
          - name: user-decrypt-euint64-req100
            dependencies: ["public-decrypt-euint64-req100"]
            template: run-test
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: test-name
                  value: "user-decrypt-euint64-req100"
                - name: test-command
                  value: "user-decrypt from-args -d euint64 -k ${KMS_KEY_ID} -e ffffffffffffffff --precompute-sns --compression --num-requests 100"


          # Summary report generation
          - name: generate-summary
            dependencies: [
              # "insecure-key-gen",
              # "insecure-crs-gen",
              "public-decrypt-euint64-req1",
              "user-decrypt-euint64-req1",
              "public-decrypt-euint64-req10",
              "user-decrypt-euint64-req10",
              "public-decrypt-euint64-req100",
              "user-decrypt-euint64-req100"
            ]
            template: summary-report
            continueOn:
              failed: true
            arguments:
              parameters:
              - name: test-result-1
                value: "{{tasks.public-decrypt-euint64-req1.outputs.parameters.test-result}}"
              - name: test-result-2
                value: "{{tasks.user-decrypt-euint64-req1.outputs.parameters.test-result}}"
              - name: test-result-3
                value: "{{tasks.public-decrypt-euint64-req10.outputs.parameters.test-result}}"
              - name: test-result-4
                value: "{{tasks.user-decrypt-euint64-req10.outputs.parameters.test-result}}"
              - name: test-result-5
                value: "{{tasks.public-decrypt-euint64-req100.outputs.parameters.test-result}}"
              - name: test-result-6
                value: "{{tasks.user-decrypt-euint64-req100.outputs.parameters.test-result}}"

    # Templates
    - name: run-test
      inputs:
        parameters:
          - name: test-name
          - name: test-command
      outputs:
        parameters:
        - name: test-result
          valueFrom:
            path: /tmp/artifacts/{{inputs.parameters.test-name}}.json
      metadata:
        labels:
          test: "{{inputs.parameters.test-name}}"
          app: kms-ci-test-suite
      script:
        image: ghcr.io/zama-ai/kms/core-client:<version>
        command: ["/bin/bash", "-c"]
        source: |
          cat <<EOF >>config.toml
          s3_endpoint="${S3_ENDPOINT}"
          object_folder=${OBJECT_FOLDER}
          core_addresses=${CORE_ADDRESSES}
          num_majority=${NUM_MAJORITY}
          num_reconstruct=${NUM_RECONSTRUCT}
          decryption_mode="${DECRYPTION_MODE}"
          fhe_params="${FHE_PARAMETER:=Test}"
          EOF

          echo "=========================================="
          echo "Starting test: {{inputs.parameters.test-command}}"
          echo "=========================================="

          # Run the specific test with timing and capture logs
          start_time=$(date +%s)
          ./bin/kms-core-client --logs -f config.toml {{inputs.parameters.test-command}} 2>&1 | tee /tmp/test_logs.txt
          test_exit=${PIPESTATUS[0]}
          end_time=$(date +%s)
          duration=$((end_time - start_time))

          # Parse performance metrics from logs
          latency_info=""
          throughput_info=""
          avg_latency=""
          median_latency=""
          min_latency=""
          max_latency=""
          throughput=""

          # Extract latency metrics from JSON logs
          if grep -q "Latency" /tmp/test_logs.txt; then
            latency_line=$(grep "Latency for" /tmp/test_logs.txt | tail -1)
            avg_latency=$(echo "$latency_line" | sed -n 's/.*Avg: \([^,]*\).*/\1/p')
            median_latency=$(echo "$latency_line" | sed -n 's/.*Median: \([^,]*\).*/\1/p')
            min_latency=$(echo "$latency_line" | sed -n 's/.*Min: \([^,]*\).*/\1/p')
            max_latency=$(echo "$latency_line" | sed -n 's/.*Max: \([^,}"]*\).*/\1/p')
            latency_info="Found latency metrics: Avg=$avg_latency, Median=$median_latency, Min=$min_latency, Max=$max_latency"
          fi

          # Extract throughput metrics
          if grep -q "Throughput:" /tmp/test_logs.txt; then
            throughput_line=$(grep "Throughput:" /tmp/test_logs.txt | tail -1)
            throughput=$(echo "$throughput_line" | sed -n 's/.*Throughput: \([0-9.]*\) requests\/s.*/\1/p')
            throughput_info="Found throughput: $throughput requests/s"
          fi

          # Report test result
          echo ""
          echo "=========================================="
          if [ $test_exit -eq 0 ]; then
            echo "‚úÖ Test '{{inputs.parameters.test-name}}' PASSED (took ${duration}s)"
            # Display performance metrics if found
            if [ -n "$latency_info" ]; then
              echo "üìä $latency_info"
            fi
            if [ -n "$throughput_info" ]; then
              echo "üöÄ $throughput_info"
            fi
          echo "=========================================="
          else
            echo "‚ùå Test '{{inputs.parameters.test-name}}' FAILED with exit code $test_exit (took ${duration}s)"
          fi

          # Save test results in JSON format for the summary report
          mkdir -p /tmp/artifacts

          # Escape any quotes in the test command to prevent JSON syntax errors
          escaped_command=$(echo "{{inputs.parameters.test-command}}" | sed 's/"/\\"/g')

          # Create JSON using printf to avoid HERE document issues
          printf '{\n  "test_name": "{{inputs.parameters.test-name}}",\n  "test_command": "%s",\n  "exit_code": %s,\n  "duration": %s,\n  "timestamp": "%s",\n  "performance_metrics": {\n    "latency": {\n      "avg": "%s",\n      "median": "%s",\n      "min": "%s",\n      "max": "%s"\n    },\n    "throughput": "%s"\n  }\n}\n' \
            "$escaped_command" \
            "$test_exit" \
            "$duration" \
            "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" \
            "$avg_latency" \
            "$median_latency" \
            "$min_latency" \
            "$max_latency" \
            "$throughput" \
            > /tmp/artifacts/{{inputs.parameters.test-name}}.json

          # Exit with the test's exit code
          exit $test_exit
        env:
          - name: S3_ENDPOINT
            valueFrom:
              configMapKeyRef:
                name: kms-ci-1
                key: CORE_CLIENT__S3_ENDPOINT
          - name: KMS_KEY_ID
            valueFrom:
              configMapKeyRef:
                name: kms-keys
                key: KMS_KEY_ID
          - name: OBJECT_FOLDER
            value: '["PUB-p1","PUB-p2","PUB-p3","PUB-p4","PUB-p5","PUB-p6","PUB-p7","PUB-p8","PUB-p9","PUB-p10","PUB-p11","PUB-p12","PUB-p13"]'
          - name: CORE_ADDRESSES
            value: '["http://kms-service-threshold-1-kms-ci-core-1:50100","http://kms-service-threshold-2-kms-ci-core-2:50100","http://kms-service-threshold-3-kms-ci-core-3:50100","http://kms-service-threshold-4-kms-ci-core-4:50100","http://kms-service-threshold-5-kms-ci-core-5:50100","http://kms-service-threshold-6-kms-ci-core-6:50100","http://kms-service-threshold-7-kms-ci-core-7:50100","http://kms-service-threshold-8-kms-ci-core-8:50100","http://kms-service-threshold-9-kms-ci-core-9:50100","http://kms-service-threshold-10-kms-ci-core-10:50100","http://kms-service-threshold-11-kms-ci-core-11:50100","http://kms-service-threshold-12-kms-ci-core-12:50100","http://kms-service-threshold-13-kms-ci-core-13:50100"]'
          - name: NUM_MAJORITY
            value: '5'
          - name: NUM_RECONSTRUCT
            value: '9'
          - name: DECRYPTION_MODE
            value: 'NoiseFloodSmall'
          - name: FHE_PARAMETER
            value: 'Default'
        resources:
          requests:
            memory: "24Gi"
            cpu: "4"
          limits:
            memory: "24Gi"
            cpu: "4"


    - name: summary-report
      inputs:
        parameters:
        - name: test-result-1
        - name: test-result-2
        - name: test-result-3
        - name: test-result-4
        - name: test-result-5
        - name: test-result-6
      script:
        image: alpine:3.16
        command: ["/bin/sh", "-c"]
        env:
        - name: WORKFLOW_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['workflows.argoproj.io/workflow']
        volumeMounts:
        - name: slack-webhook-volume
          mountPath: /etc/slack
        source: |
          apk add --no-cache jq
          # Make a directory for all the results
          mkdir -p /mnt/results
          echo '{{inputs.parameters.test-result-1}}' > /mnt/results/public-decrypt-euint64-req1.json
          echo '{{inputs.parameters.test-result-2}}' > /mnt/results/user-decrypt-euint64-req1.json
          echo '{{inputs.parameters.test-result-3}}' > /mnt/results/public-decrypt-euint64-req10.json
          echo '{{inputs.parameters.test-result-4}}' > /mnt/results/user-decrypt-euint64-req10.json
          echo '{{inputs.parameters.test-result-5}}' > /mnt/results/public-decrypt-euint64-req100.json
          echo '{{inputs.parameters.test-result-6}}' > /mnt/results/user-decrypt-euint64-req100.json
          ls -ali /mnt/results/

          echo "=========================================="
          echo "üîç KMS WITHOUT ENCLAVE TEST SUITE SUMMARY REPORT"
          echo "=========================================="
          echo "Report generated: $(date -u)"
          echo

          # Initialize counters
          total=0
          passed=0
          failed=0
          total_time=0

          echo "Test Results:"
          echo "----------------------------------------"

          # Process all result files
          for result in /mnt/results/*.json; do
            if [ -f "$result" ] && [ "$result" != "/mnt/results/summary.json" ]; then
              echo "DEBUG: Processing file: $result"
              cat $result
              echo ""

              # Try to fix common JSON syntax issues before validation
              # Fix missing comma after max field in latency object
              sed -i 's/"max": "\([^"]*\)""/"max": "\1",/g' "$result"

              echo "DEBUG: After fixing, file contents:"
              cat $result
              echo ""

              # Check if file is valid JSON before processing
              if ! jq empty "$result" 2>/dev/null; then
                echo "‚ö†Ô∏è  JSON validation failed, showing jq error:"
                jq empty "$result"
                echo "‚ö†Ô∏è  Skipping invalid JSON file: $result"
                continue
              else
                echo "‚úÖ JSON is valid"
              fi

              total=$((total + 1))

              # Extract data from the JSON file with error handling
              test_name=$(jq -r '.test_name // "unknown"' "$result" 2>/dev/null || echo "unknown")
              exit_code=$(jq -r '.exit_code // "1"' "$result" 2>/dev/null || echo "1")
              duration=$(jq -r '.duration // "0"' "$result" 2>/dev/null || echo "0")
              command=$(jq -r '.test_command // "unknown"' "$result" 2>/dev/null || echo "unknown")
              avg_latency=$(jq -r '.performance_metrics.latency.avg // empty' "$result" 2>/dev/null || echo "")
              throughput=$(jq -r '.performance_metrics.throughput // empty' "$result" 2>/dev/null || echo "")

              # Ensure duration is numeric
              if ! [[ "$duration" =~ ^[0-9]+$ ]]; then
                duration=0
              fi
              total_time=$((total_time + duration))

              # Format result line
              if [ "$exit_code" = "0" ]; then
                passed=$((passed + 1))
                perf_info=""
                if [ -n "$avg_latency" ] && [ "$avg_latency" != "null" ] && [ "$avg_latency" != "" ]; then
                  perf_info=" | Avg Latency: $avg_latency"
                fi
                if [ -n "$throughput" ] && [ "$throughput" != "null" ] && [ "$throughput" != "" ]; then
                  perf_info="$perf_info | Throughput: $throughput req/s"
                fi
                echo "‚úÖ PASS: $test_name ($duration sec$perf_info)"
              else
                failed=$((failed + 1))
                echo "‚ùå FAIL: $test_name ($duration sec)"
              fi
            fi
          done

          # Calculate success rate
          if [ $total -gt 0 ]; then
            success_rate=$(( (passed * 100) / total ))
          else
            success_rate=0
          fi

          echo "----------------------------------------"
          echo "Summary Stats:"
          echo "- Total Tests: $total"
          echo "- Passed: $passed"
          echo "- Failed: $failed"
          echo "- Success Rate: $success_rate%"
          echo "- Total Execution Time: $total_time seconds"
          echo "=========================================="

          # Create a summary file in JSON format
          cat > /mnt/results/summary.json << EOF
          {
            "total": $total,
            "passed": $passed,
            "failed": $failed,
            "success_rate": $success_rate,
            "total_time": $total_time,
            "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          }
          EOF

          # Send notification to Slack
          echo "Sending notification to Slack..."

          # Get the webhook URL from the secret
          SLACK_WEBHOOK_URL=$(cat /etc/slack/url)
          echo "DEBUG################################################"
          echo $SLACK_WEBHOOK_URL
          echo "DEBUG################################################"

          # Determine emoji based on test results
          if [ $failed -gt 0 ]; then
            STATUS_EMOJI=":x:"
            COLOR="danger"
            TITLE_EMOJI=":warning:"
          else
            STATUS_EMOJI=":white_check_mark:"
            COLOR="good"
            TITLE_EMOJI=":clap-cat:"
          fi

          # Create a formatted message for Slack
          SLACK_MESSAGE=$(cat << EOF
          {
            "attachments": [
              {
                "color": "$COLOR",
                "title": "$TITLE_EMOJI KMS WITHOUT ENCLAVE (Core-Client) Test Suite Results",
                "fields": [
                  {
                    "title": "Environment",
                    "value": "kms-ci",
                    "short": true
                  },
                  {
                    "title": "Status",
                    "value": "$passed passed, $failed failed ($success_rate% success)",
                    "short": true
                  },
                  {
                    "title": "Total Duration",
                    "value": "$total_time seconds",
                    "short": true
                  },
                  {
                    "title": "Run ID",
                    "value": "$WORKFLOW_NAME",
                    "short": true
                  },
                  {
                    "title": ":gear: Resources",
                    "value": "vCPU: 48 | Memory: 96Gi",
                    "short": true
                  },
                  {
                    "title": ":package: Versions",
                    "value": "‚Ä¢ kms-core: <version>\n",
                    "short": false
                  }
                ],
                "text": "\`\`\`\n$(for result in /mnt/results/*.json; do
                  if [ -f "$result" ] && [ "$result" != "/mnt/results/summary.json" ]; then
                    test_name=$(jq -r '.test_name' $result)
                    exit_code=$(jq -r '.exit_code' $result)
                    duration=$(jq -r '.duration' $result)
                    avg_latency=$(jq -r '.performance_metrics.latency.avg // empty' $result)
                    throughput=$(jq -r '.performance_metrics.throughput // empty' $result)

                    if [ "$exit_code" -eq "0" ]; then
                      perf_info=""
                      if [ -n "$avg_latency" ] && [ "$avg_latency" != "null" ] && [ "$avg_latency" != "" ]; then
                        perf_info="| Avg Latency: $avg_latency"
                      fi
                      if [ -n "$throughput" ] && [ "$throughput" != "null" ] && [ "$throughput" != "" ]; then
                        perf_info="$perf_info | Throughput: $throughput req/s"
                      fi
                      echo "‚úÖ PASS: $test_name ($duration sec)"
                      echo "$perf_info"
                    else
                      echo "‚ùå FAIL: $test_name ($duration sec)"
                    fi
                  fi
                done)\n\`\`\`"
              }
            ]
          }
          EOF
          )

          # Install curl
          apk add --no-cache curl
          # Send the notification
          curl -s -X POST -H "Content-type: application/json" -d "$SLACK_MESSAGE" $SLACK_WEBHOOK_URL

          echo "=========================================="
          if [ $failed -gt 0 ]; then
            echo "üìä Test Suite Summary: $passed passed, $failed failed ($success_rate% success rate)"
          else
            echo "‚úÖ Test Suite Summary: All $total tests passed! ($total_time seconds total)"
          fi
          echo "=========================================="

          # Always exit with success - summary reporting completed successfully
          exit 0

  # Credentials for pulling images
  imagePullSecrets:
  - name: registry-credentials